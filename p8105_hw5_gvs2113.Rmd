---
title: "Homework 5 - P8105 (UNI: gvs2113)"
output: html_document
date: "2023-11-07"
---

```{r setup, message= FALSE}
library(tidyverse)
library(rvest)
library(httr)
library(purrr)
```

## Problem 1 
Load in the data from github link: 
```{r}
url = "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"
homicide_html = read_csv(url)

```

The data above is a collection of criminal homicides that occurred within the 50 largest US cities over the past 10 years. This data was provided by The Washington Post and includes identifiers for each homicide, demographic information about the victim, where each homicide occurred and the disposition result. This raw data has `r homicide_html |> ncol() ` columns to describe different variables for each homicide and `r homicide_html |> nrow() ` rows for each criminal homicide entry. 

Creating a `city_state` variable and summarizing. 
```{r}
homicide_df = 
  homicide_html |> 
  mutate(
    city_state = paste(city, state, sep = ", "),
    unsolved = as.numeric(disposition %in% c("Open/No arrest", "Closed without arrest"))
    ) 

homicide_df |> 
  group_by(city_state) |> 
  summarise(total_homicide = n(),
            open_cases = sum(unsolved)) |> 
  knitr::kable(digits = 0)
```

Baltimore, MD proportion of unsolved homicides 
```{r}
baltimore_totals = 
  homicide_df |> 
  filter(city_state == "Baltimore, MD") |> 
  summarize(total_homicide = n(),
            unsolved = sum(unsolved)) |> view()
#Total Homicide = 2827 and Total Unsolved = 1825 

prop.test(x = 1825, n = 2827) |> 
  broom::tidy() |> 
  select(estimate, conf.low, conf.high)
```

Run `prop.test` on each city:
```{r}
```



## Problem 2

Creating a data frame for all participants: 
```{r, message = FALSE}
long_study = list.files(path = "./data", full.names = TRUE) |> 
  map_dfr(read_csv) |> 
  mutate(
    file_name = print(list.files(path = "./data")),
    file_name = str_remove_all(file_name, ".csv")) |>
  separate(file_name, into = c("control_arm", "subject_ID"), sep = "_") |> 
  select(subject_ID, control_arm,  everything())
```

Tidying
```{r}
tidy_long_study = 
  long_study |> 
  pivot_longer(
    week_1:week_8,
    names_to = "week_number",
    names_prefix = "week_",
    values_to = "result"
  ) |> 
  mutate(week_number = as.numeric(week_number))
```

Spaghetti Plot
```{r}
tidy_long_study |> 
  group_by(control_arm) |> 
  ggplot(aes(x = week_number, y = result, color = subject_ID)) + geom_line() + facet_grid(.~ control_arm)
```

When considering the results of the above spaghetti plots, there is a notable increase in results for the experimental arm when compared to the control arm. Also, there is an upward linear trend in results for the experimental arm over time, while the control arm have relatively negative or null trend lines. There are several control arm subjects that dip into negative results and thus, yield the minimum result values, while the experimental arm results are only ever negative at the initial week 1 results. All results for the experimental group are positive and yield the maximum results for all subjects.  

## Problem 3

```{r}
set.seed(1)

sim_function = function (n = 30, sigma = 5, mu = 0) {
  
  sim_data = tibble(
    x = rnorm(n = 30, mean = mu, sd = sigma),
    )
  
  sim_data |> 
    t.test(conf.level = 0.95) |> 
    broom::tidy() |> 
    select(estimate, p.value)
}
```

```{r}
sim_results_df = 
  expand_grid(
    mu = 0:6,
    iter = 1:100 #need to change to 5000 but dont want to crash computer
  ) |> 
  mutate(
    estimate_df = map(mu, sim_function)
  ) |> 
  unnest(estimate_df)

```

Plot of Power vs. True Value 
```{r}
#power.t.test() ##idk what to use as input, do i need another iteration?
```


Plot of Average Estimate vs. True Value 
```{r}
sim_results_df |> 
  group_by(mu) |> 
  summarize(mean_est = mean(estimate)) |> 
  ggplot(aes(x = mu, y = mean_est)) + geom_point()
```

Plot of Average Estimate (when null was rejected) vs. True Value
```{r}

```

